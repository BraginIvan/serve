def generate_grpc_client_stubs() -> None: ...
def register_model(model_name, protocol: str = "http", host: str = "localhost", port: str = "8081") -> requests.models.Response: ...
def register_workflow(workflow_name, protocol: str = "http", host: str = "localhost", port: str = "8081") -> requests.models.Response: ...
def run_inference(model_name, file_name, protocol: str = "http", host: str = "localhost", port: str = "8080", timeout: int = 120) -> requests.models.Response: ...
def start_torchserve(ncs: bool = False, model_store: str = "model_store", workflow_store: str = "", models: str = "", config_file: str = "", log_file: str = "", wait_for: int = 10, gen_mar: bool = True) -> bool: ...
def stop_torchserve(wait_for: int = 10) -> bool: ...
def unregister_model(model_name, protocol: str = "http", host: str = "localhost", port: str = "8081") -> requests.models.Response: ...
def unregister_workflow(workflow_name, protocol: str = "http", host: str = "localhost", port: str = "8081") -> requests.models.Response: ...
def workflow_prediction(workflow_name, file_name, protocol: str = "http", host: str = "localhost", port: str = "8080", timeout: int = 120) -> requests.models.Response: ...
